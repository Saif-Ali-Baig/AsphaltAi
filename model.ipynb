{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393e1c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROAD REPAIR PREDICTION - ENHANCED SYNTHETIC DATA, HYBRID MODELING, IMPROVED ACCURACY\n",
    "!pip install -r requirements.txt -q\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate a larger synthetic dataset\n",
    "n_samples = 5000\n",
    "materials = ['asphalt', 'concrete', 'gravel']\n",
    "weather_types = ['hot', 'humid', 'rainy', 'dry', 'cold']\n",
    "usage_types = ['residential', 'commercial', 'highway']\n",
    "traffic_levels = ['low', 'medium', 'high']\n",
    "\n",
    "data = {\n",
    "    'last_laid_year': np.random.randint(1980, 2020, size=n_samples),\n",
    "    'last_repair_year': np.random.randint(1985, 2025, size=n_samples),\n",
    "    'material': np.random.choice(materials, size=n_samples),\n",
    "    'weather': np.random.choice(weather_types, size=n_samples),\n",
    "    'usage_type': np.random.choice(usage_types, size=n_samples),\n",
    "    'traffic_level': np.random.choice(traffic_levels, size=n_samples),\n",
    "    'accidents_reported': np.random.randint(0, 30, size=n_samples)\n",
    "}\n",
    "\n",
    "# Assuming 'data' is your dataset\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate road age and years since last repair\n",
    "df['road_age'] = 2025 - df['last_laid_year']\n",
    "df['years_since_repair'] = 2025 - df['last_repair_year']\n",
    "\n",
    "# Ensure that road_age is always greater than years_since_repair\n",
    "df['road_age'] = df.apply(lambda row: max(row['road_age'], row['years_since_repair'] + 1), axis=1)\n",
    "\n",
    "# Now, road_age is guaranteed to be greater than years_since_repair\n",
    "\n",
    "df.to_csv('synthetic_data.csv', index=False)\n",
    "#df.head()\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the synthetic_data.csv\n",
    "df_synthetic = pd.read_csv('synthetic_data.csv')\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# List of categorical columns to encode (same as before)\n",
    "categorical_cols = ['material', 'weather', 'usage_type', 'traffic_level']\n",
    "\n",
    "# Encode each categorical column in the synthetic data\n",
    "for col in categorical_cols:\n",
    "    df_synthetic[col] = encoder.fit_transform(df_synthetic[col])\n",
    "\n",
    "# Save the encoded synthetic data\n",
    "df_synthetic.to_csv('synthetic_data_encoded.csv', index=False)\n",
    "df_synthetic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f06039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# ... (Code for generating 'last_laid_year', 'last_repair_year', 'material', etc. - same as before) ...\n",
    "\n",
    "# Calculate road age and years since last repair (same as before)\n",
    "df['road_age'] = 2025 - df['last_laid_year']\n",
    "df['years_since_repair'] = 2025 - df['last_repair_year']\n",
    "df['road_age'] = df.apply(lambda row: max(row['road_age'], row['years_since_repair'] + 1), axis=1)\n",
    "\n",
    "# --- Introduce Noise and Complex Patterns ---\n",
    "\n",
    "# 1. Encode categorical features for numerical calculations\n",
    "material_map = {'asphalt': 0, 'concrete': 1, 'gravel': 2}\n",
    "weather_map = {'hot': 0, 'humid': 1, 'rainy': 2, 'dry': 3, 'cold': 4}\n",
    "usage_map = {'residential': 0, 'commercial': 1, 'highway': 2}\n",
    "traffic_map = {'low': 0, 'medium': 1, 'high': 2}\n",
    "\n",
    "df['material_encoded'] = df['material'].map(material_map)\n",
    "df['weather_encoded'] = df['weather'].map(weather_map)\n",
    "df['usage_encoded'] = df['usage_type'].map(usage_map)\n",
    "df['traffic_encoded'] = df['traffic_level'].map(traffic_map)\n",
    "\n",
    "# 2. Create a complex factor using encoded features and noise\n",
    "# Use len(df) instead of n_samples to match the DataFrame size\n",
    "complex_factor = (\n",
    "    0.3 * df['road_age'] +\n",
    "    0.2 * df['years_since_repair'] +\n",
    "    0.1 * df['material_encoded'] +\n",
    "    0.15 * df['weather_encoded'] +\n",
    "    0.1 * df['usage_encoded'] +\n",
    "    0.15 * df['traffic_encoded'] +\n",
    "    0.2 * (df['accidents_reported'] / 30) +  # Scale accidents\n",
    "    np.random.normal(0, 0.2, len(df))  # Add Gaussian noise - Changed n_samples to len(df)\n",
    ")\n",
    "\n",
    "# 3. Label logic (thresholding based on complex factor)\n",
    "df['needs_repair'] = (complex_factor > 1.5).astype(int)  # Adjust threshold as needed\n",
    "\n",
    "# --- Cleanup ---\n",
    "df = df.drop(columns=['material_encoded', 'weather_encoded', 'usage_encoded', 'traffic_encoded'])  # Remove encoded columns\n",
    "\n",
    "#df.to_csv('noisy_test_data.csv', index=False)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of samples\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate features (same as before)\n",
    "# ... (Code for generating 'last_laid_year', 'last_repair_year', 'material', etc.) ...\n",
    "\n",
    "# --- Introduce Noise and Complex Patterns ---\n",
    "\n",
    "# 1. Numerical Features: Add noise scaled by feature's standard deviation\n",
    "numerical_cols = ['road_age', 'years_since_repair', 'accidents_reported']\n",
    "for col in numerical_cols:\n",
    "    noise_scale = 0.1  # Adjust the noise scale (e.g., 0.1 for 10% noise)\n",
    "    noise = np.random.normal(0, noise_scale * df[col].std(), size=len(df))\n",
    "    df[col] = df[col] + noise\n",
    "\n",
    "# 2. Categorical Features: Introduce random flips with a low probability\n",
    "categorical_cols = ['material', 'weather', 'usage_type', 'traffic_level']\n",
    "flip_probability = 0.05  # Adjust the flip probability (e.g., 0.05 for 5% flips)\n",
    "for col in categorical_cols:\n",
    "    flip_indices = np.random.choice(df.index, size=int(flip_probability * len(df)), replace=False)\n",
    "    # Iterate through the flip indices to get unique values excluding current value at each index\n",
    "    for i in flip_indices:\n",
    "        # Get unique values in the column (excluding the current value at the index 'i')\n",
    "        unique_values = [val for val in df[col].unique() if val != df.loc[i, col]]\n",
    "        # Choose a new random value for the current index 'i'\n",
    "        new_value = np.random.choice(unique_values, size=1)[0]\n",
    "        df.loc[i, col] = new_value\n",
    "\n",
    "# ... (rest of the code, including calculating road_age, years_since_repair, and generating needs_repair) ...\n",
    "\n",
    "# --- Save the modified data ---\n",
    "#df.to_csv('noisy_test_data_with_noise.csv', index=False)\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "#df = pd.read_csv('noisy_test_data_with_noise.csv')\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# List of categorical columns to encode\n",
    "categorical_cols = ['material', 'weather', 'usage_type', 'traffic_level']\n",
    "\n",
    "# Encode each categorical column\n",
    "for col in categorical_cols:\n",
    "    df[col] = encoder.fit_transform(df[col])\n",
    "\n",
    "# Save the encoded data\n",
    "df.to_csv('noisy_test_data_encoded.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32284225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "dataset_1 = pd.read_csv('synthetic_data_encoded.csv')\n",
    "dataset_2 = pd.read_csv('noisy_test_data_encoded.csv')\n",
    "\n",
    "# Convert 'road_age' and 'years_since_repair' to integers\n",
    "dataset_1['road_age'] = dataset_1['road_age'].astype(int)\n",
    "dataset_1['years_since_repair'] = dataset_1['years_since_repair'].astype(int)\n",
    "dataset_2['road_age'] = dataset_2['road_age'].astype(int)\n",
    "dataset_2['years_since_repair'] = dataset_2['years_since_repair'].astype(int)\n",
    "\n",
    "# Remove 'needs_repair' column from both datasets before concatenating\n",
    "#dataset_1 = dataset_1.drop('needs_repair', axis=1)\n",
    "dataset_2 = dataset_2.drop('needs_repair', axis=1)\n",
    "\n",
    "# Concatenate the datasets\n",
    "combined_dataset = pd.concat([dataset_1, dataset_2], ignore_index=True)\n",
    "\n",
    "# Shuffle the rows\n",
    "combined_dataset = combined_dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "combined_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b11496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'combined_dataset' is your latest merged DataFrame (without 'needs_repair')\n",
    "\n",
    "# --- Define more complex logic to generate 'needs_repair' ---\n",
    "def predict_needs_repair(row):\n",
    "    # Base condition: road age and years since last repair\n",
    "    base_condition = (row['road_age'] > 25) | (row['years_since_repair'] > 12)\n",
    "\n",
    "    # Material-specific conditions\n",
    "    material_condition = False\n",
    "    if row['material'] == 0:  # Assuming 0 represents 'asphalt' (adjust based on encoding)\n",
    "        material_condition = row['road_age'] > 20  # Asphalt roads degrade faster\n",
    "    elif row['material'] == 2:  # Assuming 2 represents 'gravel'\n",
    "        material_condition = row['years_since_repair'] > 8  # Gravel roads need more frequent repair\n",
    "\n",
    "    # Weather-specific conditions\n",
    "    weather_condition = False\n",
    "    if row['weather'] == 2:  # Assuming 2 represents 'rainy'\n",
    "        weather_condition = row['years_since_repair'] > 10  # Rainy weather increases degradation\n",
    "    elif row['weather'] == 0:  # Assuming 0 represents 'hot'\n",
    "        weather_condition = row['road_age'] > 22  # Hot weather can damage asphalt\n",
    "\n",
    "    # Usage and traffic conditions\n",
    "    usage_traffic_condition = False\n",
    "    if row['usage_type'] == 2 and row['traffic_level'] == 2:  # Assuming 2 represents 'highway' and 'high' traffic\n",
    "        usage_traffic_condition = row['years_since_repair'] > 7  # Highways with high traffic degrade faster\n",
    "\n",
    "    # Accidents condition\n",
    "    accident_condition = row['accidents_reported'] > 5  # More accidents might indicate road damage\n",
    "\n",
    "    # Combine conditions with logical OR\n",
    "    needs_repair = base_condition | material_condition | weather_condition | usage_traffic_condition | accident_condition\n",
    "    return int(needs_repair)  # Convert to 0/1\n",
    "\n",
    "combined_dataset['needs_repair'] = combined_dataset.apply(predict_needs_repair, axis=1)\n",
    "\n",
    "# --- Now split the data ---\n",
    "X = combined_dataset.drop('needs_repair', axis=1)  # Features\n",
    "y = combined_dataset['needs_repair']  # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b557c8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow -q\n",
    "!pip install xgboost -q\n",
    "!pip install scikeras -q\n",
    "!pip install tensorflow_hub -q\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Reshape, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from scikeras.wrappers import KerasClassifier  # For wrapping Keras models in scikit-learn\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss  # For calculating log loss\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# ... (Your code for loading and preprocessing datasets - same as before) ...\n",
    "\n",
    "# Load datasets\n",
    "dataset_1 = pd.read_csv('synthetic_data_encoded.csv')\n",
    "dataset_2 = pd.read_csv('noisy_test_data_encoded.csv')\n",
    "\n",
    "# Convert 'road_age' and 'years_since_repair' to integers\n",
    "dataset_1['road_age'] = dataset_1['road_age'].astype(int)\n",
    "dataset_1['years_since_repair'] = dataset_1['years_since_repair'].astype(int)\n",
    "dataset_2['road_age'] = dataset_2['road_age'].astype(int)\n",
    "dataset_2['years_since_repair'] = dataset_2['years_since_repair'].astype(int)\n",
    "\n",
    "# Remove 'needs_repair' column from both datasets before concatenating\n",
    "#dataset_1 = dataset_1.drop('needs_repair', axis=1)\n",
    "dataset_2 = dataset_2.drop('needs_repair', axis=1)\n",
    "\n",
    "# Concatenate the datasets\n",
    "combined_dataset = pd.concat([dataset_1, dataset_2], ignore_index=True)\n",
    "\n",
    "# Shuffle the rows\n",
    "combined_dataset = combined_dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# --- Drop 'last_laid_year' and 'last_repair_year' ---\n",
    "combined_dataset = combined_dataset.drop(['last_laid_year', 'last_repair_year'], axis=1)\n",
    "\n",
    "# Separate features and labels\n",
    "#X = combined_dataset.drop('needs_repair', axis=1)\n",
    "#y = combined_dataset['needs_repair']\n",
    "\n",
    "def predict_needs_repair(row):\n",
    "    # Base condition: road age and years since last repair\n",
    "    base_condition = (row['road_age'] > 25) | (row['years_since_repair'] > 12)\n",
    "\n",
    "    # Material-specific conditions\n",
    "    material_condition = False\n",
    "    if row['material'] == 0:  # Assuming 0 represents 'asphalt' (adjust based on encoding)\n",
    "        material_condition = row['road_age'] > 20  # Asphalt roads degrade faster\n",
    "    elif row['material'] == 2:  # Assuming 2 represents 'gravel'\n",
    "        material_condition = row['years_since_repair'] > 8  # Gravel roads need more frequent repair\n",
    "\n",
    "    # Weather-specific conditions\n",
    "    weather_condition = False\n",
    "    if row['weather'] == 2:  # Assuming 2 represents 'rainy'\n",
    "        weather_condition = row['years_since_repair'] > 10  # Rainy weather increases degradation\n",
    "    elif row['weather'] == 0:  # Assuming 0 represents 'hot'\n",
    "        weather_condition = row['road_age'] > 22  # Hot weather can damage asphalt\n",
    "\n",
    "    # Usage and traffic conditions\n",
    "    usage_traffic_condition = False\n",
    "    if row['usage_type'] == 2 and row['traffic_level'] == 2:  # Assuming 2 represents 'highway' and 'high' traffic\n",
    "        usage_traffic_condition = row['years_since_repair'] > 7  # Highways with high traffic degrade faster\n",
    "\n",
    "    # Accidents condition\n",
    "    accident_condition = row['accidents_reported'] > 5  # More accidents might indicate road damage\n",
    "\n",
    "    # Combine conditions with logical OR\n",
    "    needs_repair = base_condition | material_condition | weather_condition | usage_traffic_condition | accident_condition\n",
    "    return int(needs_repair)  # Convert to 0/1\n",
    "\n",
    "# Apply the logic to generate 'needs_repair' column\n",
    "combined_dataset['needs_repair'] = combined_dataset.apply(predict_needs_repair, axis=1)\n",
    "\n",
    "# Separate features and labels\n",
    "X = combined_dataset.drop('needs_repair', axis=1)\n",
    "y = combined_dataset['needs_repair']\n",
    "\n",
    "# --- Preprocessing ---\n",
    "# 1. Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# 2. One-hot encode categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') # Create OneHotEncoder instance\n",
    "encoded_data = encoder.fit_transform(X[categorical_cols]) # Fit and transform on categorical data\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_cols)) # Create DataFrame from encoded data\n",
    "X = X.drop(categorical_cols, axis=1) # Drop original categorical columns\n",
    "X = pd.concat([X, encoded_df], axis=1) # Concatenate encoded features\n",
    "\n",
    "# 3. Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# --- Define and Train Models ---\n",
    "\n",
    "# 1. Standalone Neural Network\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "hidden_layer1 = Dense(32, activation='relu')(input_layer)\n",
    "dropout1 = Dropout(0.3)(hidden_layer1)\n",
    "hidden_layer2 = Dense(16, activation='relu')(dropout1)\n",
    "dropout2 = Dropout(0.3)(hidden_layer2)\n",
    "output_layer = Dense(1, activation='sigmoid')(dropout2)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=64,\n",
    "                    validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# 2. Hybrid KNN+NN\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_features_train = knn.kneighbors(X_train, return_distance=False)\n",
    "knn_features_test = knn.kneighbors(X_test, return_distance=False)\n",
    "\n",
    "input_layer_hybrid = Input(shape=(knn_features_train.shape[1],))\n",
    "hidden_layer_hybrid = Dense(32, activation='relu')(input_layer_hybrid)\n",
    "dropout_layer_hybrid = Dropout(0.5)(hidden_layer_hybrid)\n",
    "output_layer_hybrid = Dense(1, activation='sigmoid')(dropout_layer_hybrid)\n",
    "\n",
    "hybrid_model = Model(inputs=input_layer_hybrid, outputs=output_layer_hybrid)\n",
    "hybrid_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "hybrid_model.fit(knn_features_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n",
    "# 3. Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 4. XGBoost\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# 6. AdaBoost\n",
    "ada_model = AdaBoostClassifier(random_state=42)\n",
    "ada_model.fit(X_train, y_train)\n",
    "\n",
    "# 7. Support Vector Machine\n",
    "svm_model = SVC(probability=True, random_state=42)  # probability=True for predict_proba\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# 8. Logistic Regression\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# 9. Pre-trained TensorFlow Model (example using EfficientNetB0)\n",
    "# Load pre-trained model\n",
    "#base_model = hub.KerasLayer(\"https://tfhub.dev/tensorflow/efficientnet/b0/classification/1\",\n",
    "                         trainable=False)  # Set trainable=True if you want to fine-tune\n",
    "\n",
    "# Create a new model on top of the pre-trained model\n",
    "'''input_tensor = Input(shape=(X_train.shape[1],)) # Assuming your input shape\n",
    "x = Reshape((1, 1, X_train.shape[1]))(input_tensor) # Reshape to (1, 1, num_features)\n",
    "efficientnet_output = base_model(x)\n",
    "efficientnet_output = GlobalAveragePooling2D()(efficientnet_output) # Add GlobalAveragePooling2D\n",
    "output_tensor = Dense(1, activation='sigmoid')(efficientnet_output)\n",
    "\n",
    "efficientnet_model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "efficientnet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "efficientnet_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "'''\n",
    "\n",
    "# --- Evaluate Models and Calculate Loss ---\n",
    "\n",
    "models = {\n",
    "    \"Standalone NN\": model,\n",
    "    \"Hybrid KNN+NN\": hybrid_model,\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"XGBoost\": xgb_model,\n",
    "    \"Gradient Boosting\": gb_model,\n",
    "    \"AdaBoost\": ada_model,\n",
    "    \"SVM\": svm_model,\n",
    "    \"Logistic Regression\": lr_model,\n",
    "     #\"EfficientNetB0\": efficientnet_model,  # Uncomment if using\n",
    "    # \"Hybrid KNN+MobileNetV2\": hybrid_mobilenet_model  # Uncomment if using\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    if model_name in [\"Standalone NN\", \"Hybrid KNN+NN\", \"EfficientNetB0\"]:  # Keras models\n",
    "        loss, accuracy = model.evaluate(X_test if model_name == \"Standalone NN\" else knn_features_test if model_name == \"Hybrid KNN+NN\" else X_test , y_test, verbose=0)\n",
    "    else:  # Scikit-learn models\n",
    "        predictions = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        loss = log_loss(y_test, model.predict_proba(X_test)[:, 1])  # Calculate log loss for binary classification\n",
    "\n",
    "    results[model_name] = {\"Accuracy\": accuracy, \"Loss\": loss}\n",
    "\n",
    "# --- Print Results ---\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name} - Test Accuracy: {metrics['Accuracy']:.4f}, Test Loss: {metrics['Loss']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
